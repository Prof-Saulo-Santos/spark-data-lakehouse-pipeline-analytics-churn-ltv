# Manual Completo de Implementa√ß√£o: Data Lakehouse & ML Analytics
**Projeto: Customer Intelligence 360**

Este manual re√∫ne todas as fases de implementa√ß√£o do projeto, desde a configura√ß√£o da infraestrutura at√© o treinamento dos modelos de Machine Learning. Ele foi desenhado para ser autossuficiente: cont√©m tanto as explica√ß√µes te√≥ricas quanto o c√≥digo-fonte integral, permitindo que qualquer engenheiro reproduza o ambiente do zero.

---

# Cap√≠tulo 1: Configura√ß√£o, Infraestrutura e Planejamento

## 1.1 Objetivo
Estabelecer uma funda√ß√£o s√≥lida para o projeto. Em vez de scripts soltos, foi criada uma estrutura profissional que suporta processamento em larga escala (Spark), armazenamento confi√°vel (Delta Lake) e observabilidade (Logs estruturados).

## 1.2 Plano de Execu√ß√£o & Setup Inicial
O fluxo segue a metodologia de "Fail-Fast" e desenvolvimento iterativo. Antes da codifica√ß√£o, √© preciso ter a "casa arrumada".

### Pr√©-requisitos
*   **Python 3.10+**: `python3 --version`
*   **Git**: `git --version`
*   **Poetry**: `pipx install poetry` (Gerenciador de depend√™ncias moderno)

### Passo 1: Inicializa√ß√£o do Git
```bash
# Na raiz do projeto
git init
git branch -M main
```

### Passo 2: Estrutura de Diret√≥rios
Antes da configura√ß√£o, deve-se criar a estrutura de pastas e os arquivos `__init__.py` para que o Python reconhe√ßa os pacotes.

```bash
mkdir -p src/datagen src/jobs src/lib src/ml
touch src/__init__.py src/datagen/__init__.py src/jobs/__init__.py src/lib/__init__.py src/ml/__init__.py
```

### Passo 3: Arquivo `.gitignore`
Essencial para n√£o commitar lixo (logs, dados tempor√°rios, ambientes virtuais).

```gitignore
__pycache__/
*.py[cod]
.venv/
poetry.lock
.env
.pytest_cache/
.coverage
htmlcov/
dist/
.DS_Store

# Data Lake Local (N√£o versionar dados massivos!)
data/bronze/*
data/silver/*
data/gold/*
!data/**/.gitkeep

# Spark/Metastore
metastore_db/
derby.log
spark-warehouse/
```

### Passo 4: Gerenciamento de Depend√™ncias (`pyproject.toml`)
Este arquivo define o projeto e suas bibliotecas. Note que as vers√µes abaixo s√£o as testadas para este projeto.

```toml
[tool.poetry]
name = "customer-intelligence-360"
version = "0.1.0"
description = "Predi√ß√£o de Churn e LTV em escala com Spark, Delta Lake e MLOps"
authors = ["Engenheiro de Dados <seu@email.com>"]
readme = "README.md"
packages = [{include = "src"}]

[tool.poetry.dependencies]
python = ">=3.10,<3.13"
# Core
pyspark = "3.5.3"
delta-spark = "3.2.1"
# Local Data
pandas = "2.1.4"
numpy = "1.26.4"
# Generation & Quality
Faker = "30.8.2"
great-expectations = "1.3.14"
# Models
mlflow = "2.19.0"
scikit-learn = "1.5.2"
xgboost = "2.1.3"
lifetimes = "0.11.3"
# Utils
click = "8.1.8"
python-dotenv = "1.0.1"
setuptools = "^80.9.0"

[tool.poetry.group.dev.dependencies]
pytest = "8.3.4"
bump2version = "1.0.1"
pre-commit = "4.0.1"
black = "24.10.0"
ruff = "0.8.3"

[tool.poetry.scripts]
ci360-datagen = "src.datagen.main:main"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
```

### Passo 5: Configura√ß√£o de Versionamento (`.bumpversion.cfg`)
Automatiza o incremento de vers√£o (ex: `v0.1.0` -> `v0.2.0`) em todos os arquivos necess√°rios.

```ini
[bumpversion]
current_version = 0.1.0
commit = True
tag = True
tag_name = v{new_version}

[bumpversion:file:pyproject.toml]
search = version = "{current_version}"
replace = version = "{new_version}"

[bumpversion:file:src/lib/__init__.py]
search = __version__ = "{current_version}"
replace = __version__ = "{new_version}"
```

### Passo 6: Automa√ß√£o (`Makefile`)
Crie um arquivo `Makefile` para criar atalhos de comando.

```makefile
.PHONY: setup dataset etl train clean bump-patch bump-minor bump-major

setup:
	pip install poetry
	poetry install

dataset:
	poetry run python src/datagen/main.py --rows 1000000

etl:
	poetry run python src/jobs/ingestion_bronze.py
	poetry run python src/jobs/refine_silver.py
	poetry run python src/jobs/agg_gold.py

train:
	poetry run python src/ml/train_churn.py

clean:
	rm -rf data/bronze/*
	rm -rf data/silver/*
	rm -rf data/gold/*
	rm -rf spark-warehouse
	rm -rf .venv
	find . -type d -name "__pycache__" -exec rm -rf {} +

bump-patch:
	poetry run bump2version patch --verbose

bump-minor:
	poetry run bump2version minor --verbose

bump-major:
	poetry run bump2version major --verbose
```


### Passo 7: Documenta√ß√£o (`README.md`)
O Poetry exige um arquivo `README.md`. Crie-o na raiz com o seguinte conte√∫do:

```markdown
# Customer Intelligence 360

```

### üöÄ Passo 8: Instala√ß√£o!

Com todos os arquivos de configura√ß√£o criados (`pyproject.toml`, `Makefile`, `README.md`, etc.), a funda√ß√£o est√° pronta. Agora, com a configura√ß√£o finalizada, ver-se-√° a m√°gica acontecer.

**Abrir o terminal na raiz do projeto e executar:**

```bash
make setup
```

Este comando vai acionar o Poetry, criar o ambiente virtual isolado e baixar todas as bibliotecas pesadas (Spark, Delta Lake, XGBoost).
Se forem vistas v√°rias barras de progresso terminando com sucesso... Parab√©ns! üéâ
O ambiente de Engenharia de Dados est√° pronto.

Agora, **segue-se para a codifica√ß√£o** dos scripts:

1. src/obs/test_delta.py
2. src/lib/spark_utils.py
3. src/lib/logger.py
---
### üß™ Passo 9: Teste de Fuma√ßa (Smoke Test)

Antes de gerar terabytes de dados, garanta que o Spark conseguir√° escrever no disco.
Crie o arquivo `src/jobs/test_delta.py` com o conte√∫do abaixo:

```python
# -------------------------
# Imports de bibliotecas
# -------------------------

# sys √© usado para encerrar o processo com c√≥digo de erro em caso de falha
import sys

# SparkSession √© o ponto de entrada para aplica√ß√µes Spark
from pyspark.sql import SparkSession


def main():
    """
    Script de teste para validar se o ambiente Spark est√° corretamente
    configurado com suporte ao Delta Lake.

    Objetivo:
    - Inicializar uma SparkSession com Delta Lake habilitado
    - Criar um DataFrame simples
    - Escrever e ler uma tabela Delta
    - Confirmar que o ambiente est√° operacional
    """
    
    # Mensagem inicial de teste
    print("Testing Spark + Delta Lake...")
    
    try:
        # -------------------------
        # Cria√ß√£o da SparkSession com suporte ao Delta Lake
        # -------------------------
        spark = (
            SparkSession.builder
            # Nome da aplica√ß√£o Spark
            .appName("DeltaTest")
            
            # Pacote do Delta Lake compat√≠vel com Scala 2.12
            .config(
                "spark.jars.packages",
                "io.delta:delta-spark_2.12:3.2.1"
            )
            
            # Extens√£o necess√°ria para habilitar comandos SQL do Delta
            .config(
                "spark.sql.extensions",
                "io.delta.sql.DeltaSparkSessionExtension"
            )
            
            # Cat√°logo padr√£o do Spark apontando para o Delta
            .config(
                "spark.sql.catalog.spark_catalog",
                "org.apache.spark.sql.delta.catalog.DeltaCatalog"
            )
            
            # Cria ou reutiliza a SparkSession
            .getOrCreate()
        )
        
        # -------------------------
        # Cria√ß√£o de um DataFrame simples para teste
        # -------------------------
        data = [
            ("Alice", 1),
            ("Bob", 2)
        ]
        
        # Cria o DataFrame com schema impl√≠cito
        df = spark.createDataFrame(
            data,
            ["name", "id"]
        )
        
        # -------------------------
        # Escrita do DataFrame em formato Delta
        # -------------------------
        print("Writing Delta Table...")
        
        df.write \
            .format("delta") \
            .mode("overwrite") \
            .save("/tmp/delta-test-table")
        
        # -------------------------
        # Leitura da tabela Delta
        # -------------------------
        print("Reading Delta Table...")
        
        df_read = (
            spark.read
            .format("delta")
            .load("/tmp/delta-test-table")
        )
        
        # Exibe o conte√∫do lido
        df_read.show()
        
        # Se tudo ocorreu corretamente
        print("SUCCESS: Spark + Delta operational!")
        
    except Exception as e:
        # Em caso de erro, exibe a mensagem e encerra com c√≥digo != 0
        print(f"FAILURE: {e}")
        sys.exit(1)


# -------------------------
# Ponto de entrada do script
# -------------------------
if __name__ == "__main__":
    main()
```

**Executar o teste:**
```bash
poetry run python src/jobs/test_delta.py
```

Se aparecer `SUCCESS: Spark + Delta operational!`, voc√™ est√° aprovado para a Fase 2.

## 1.3 Estrutura de Diret√≥rios

A organiza√ß√£o das pastas segue o padr√£o de projetos de Engenharia de Dados modernos, separando c√≥digo, dados e configura√ß√£o.

```text
raiz/
‚îú‚îÄ‚îÄ data/               # Armazenamento do Lakehouse (Local)
‚îÇ   ‚îú‚îÄ‚îÄ bronze/         # Dados brutos (Delta Lake)
‚îÇ   ‚îú‚îÄ‚îÄ silver/         # Dados limpos e deduplicados
‚îÇ   ‚îî‚îÄ‚îÄ gold/           # Dados agregados para Analytics/ML
‚îú‚îÄ‚îÄ docs/               # Documenta√ß√£o do projeto
‚îú‚îÄ‚îÄ src/                # C√≥digo-fonte
‚îÇ   ‚îú‚îÄ‚îÄ datagen/        # Scripts de gera√ß√£o de dados
‚îÇ   ‚îú‚îÄ‚îÄ jobs/           # Jobs Spark (ETL)
‚îÇ   ‚îú‚îÄ‚îÄ lib/            # Utilit√°rios compartilhados (Logger, Spark Session)
‚îÇ   ‚îî‚îÄ‚îÄ ml/             # Scripts de treinamento de modelos
‚îú‚îÄ‚îÄ tests/              # Testes unit√°rios e de integra√ß√£o
‚îú‚îÄ‚îÄ mlruns/             # Rastreamento de experimentos (MLflow)
‚îú‚îÄ‚îÄ poetry.lock         # Vers√µes exatas das depend√™ncias
‚îú‚îÄ‚îÄ pyproject.toml      # Defini√ß√£o do projeto e depend√™ncias
‚îî‚îÄ‚îÄ Makefile            # Atalhos para comandos comuns
```

## 1.4 Infraestrutura de C√≥digo Base

Estes arquivos "lib" s√£o os alicerces do projeto. Eles abstraem a complexidade de configura√ß√£o do Spark e padronizam o logging.

### `src/lib/spark_utils.py`
**O que faz:** Cria e devolve uma `SparkSession` pronta para uso.
**Por que √© necess√°rio:** Configurar o Delta Lake envolve v√°rias flags (`spark.sql.extensions`, `spark.jars.packages`). Centralizar isso evita bugs de configura√ß√£o e repeti√ß√£o de c√≥digo.
**Destaques:**
*   **Delta Integration**: As configs `io.delta` habilitam o Delta Lake (ACID transactions).
*   **Local Mode**: `master("local[*]")` instrui o Spark a usar todos os n√∫cleos de CPU dispon√≠veis na sua m√°quina.

```python
from pyspark.sql import SparkSession
import os

def get_spark_session(app_name: str, local: bool = True) -> SparkSession:
    """
    Creates or gets a Spark Session configured for Delta Lake.
    """
    builder = SparkSession.builder \
        .appName(app_name) \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .config("spark.jars.packages", "io.delta:delta-spark_2.12:3.2.1")

    if local:
        # Optimization for local mode
        builder = builder \
            .master("local[*]") \
            .config("spark.driver.memory", "2g")
            
    spark = builder.getOrCreate()
    
    # Adjust log level to reduce noise
    spark.sparkContext.setLogLevel("WARN")
    
    return spark
```

### `src/lib/logger.py`
**O que faz:** Padroniza a sa√≠da de logs em formato JSON.
**Por que √© necess√°rio:** Em produ√ß√£o, logs de texto puro s√£o dif√≠ceis de parsear. Logs JSON s√£o ingeridos facilmente por ferramentas como Datadog, Splunk ou CloudWatch (AWS).

```python
# -------------------------
# Imports de bibliotecas padr√£o
# -------------------------

# M√≥dulo padr√£o de logging do Python
import logging

# JSON para serializa√ß√£o estruturada dos logs
import json

# Socket para obter informa√ß√µes da m√°quina (hostname)
import socket

# Datetime para registro preciso do hor√°rio do evento
from datetime import datetime


class StructuredLogger:
    """
    Logger estruturado que gera logs em formato JSON.

    Objetivos:
    - Facilitar ingest√£o por sistemas de observabilidade (ELK, OpenSearch, Datadog)
    - Padronizar logs em pipelines de dados e MLOps
    - Permitir inclus√£o de metadados contextuais (ex: table_name, rows, source)
    """

    def __init__(self, name="CI360"):
        """
        Inicializa o logger estruturado.

        Par√¢metros:
        - name: nome l√≥gico do logger (normalmente o nome do servi√ßo ou pipeline)
        """
        
        # Cria ou recupera um logger com o nome especificado
        self.logger = logging.getLogger(name)
        
        # Define o n√≠vel m√≠nimo de log (INFO)
        self.logger.setLevel(logging.INFO)
        
        # Handler para sa√≠da em console (stdout)
        handler = logging.StreamHandler()
        
        # Formatter simples: a mensagem j√° estar√° em JSON
        formatter = logging.Formatter('%(message)s')
        handler.setFormatter(formatter)
        
        # Evita adicionar m√∫ltiplos handlers ao mesmo logger
        # (comum em execu√ß√µes repetidas ou testes)
        if not self.logger.handlers:
            self.logger.addHandler(handler)
        
        # Captura o hostname da m√°quina para rastreabilidade
        self.hostname = socket.gethostname()

    def _format(self, level, message, **kwargs):
        """
        Formata a mensagem de log em um objeto JSON.

        Campos padr√£o:
        - timestamp: data/hora em UTC
        - level: n√≠vel do log (INFO, ERROR, WARNING)
        - host: hostname da m√°quina
        - message: mensagem principal
        - kwargs: metadados adicionais fornecidos na chamada
        """
        
        log_entry = {
            # Timestamp em UTC para padroniza√ß√£o em ambientes distribu√≠dos
            "timestamp": datetime.utcnow().isoformat(),
            
            # N√≠vel do log
            "level": level,
            
            # Host onde o processo est√° rodando
            "host": self.hostname,
            
            # Mensagem principal
            "message": message,
            
            # Metadados adicionais (ex: tabela, linhas, caminho)
            **kwargs
        }
        
        # Converte o dicion√°rio para string JSON
        return json.dumps(log_entry)

    def info(self, message, **kwargs):
        """
        Registra uma mensagem de log no n√≠vel INFO.
        """
        self.logger.info(
            self._format("INFO", message, **kwargs)
        )

    def error(self, message, **kwargs):
        """
        Registra uma mensagem de log no n√≠vel ERROR.
        """
        self.logger.error(
            self._format("ERROR", message, **kwargs)
        )
    
    def warning(self, message, **kwargs):
        """
        Registra uma mensagem de log no n√≠vel WARNING.
        """
        self.logger.warning(
            self._format("WARNING", message, **kwargs)
        )

```


### üõë Checkpoint: Infraestrutura Base (v0.1.0)

Com a infraestrutura configurada e testada, ser√° salva a primeira vers√£o do projeto. Esta vers√£o (`v0.1.0`) representa o ambiente base est√°vel.

1.  **Commitar Infraestrutura:**
    ```bash
    git add .
    git commit -m "build: setup project infrastructure"
    ```

2.  **Tag Inicial:**
    Como a vers√£o `0.1.0` j√° foi definida no `pyproject.toml`, ela ser√° oficializada no Git:
    ```bash
    git tag v0.1.0
    ```

3.  **Sincronizar com Reposit√≥rio Remoto:**
    Troque o link abaixo pelo link do seu reposit√≥rio:
    ```bash
    git remote add origin https://github.com/Prof-Saulo-Santos/spark-data-lakehouse-pipeline-analytics-churn-ltv
    git push -u origin main
    git push --tags
    ```

---

# Cap√≠tulo 2: Gera√ß√£o de Dados (Fase 2)

## 2.1 Vis√£o Geral
Como n√£o h√° acesso a dados reais de clientes (LGPD/GDPR), foi criado um gerador de dados sint√©ticos.
**Objetivo:** Gerar arquivos `.parquet` contendo hist√≥rico de navega√ß√£o e compras.
**Ferramenta:** `Faker` (biblioteca Python para gerar nomes, emails e datas falsas mas realistas).

## 2.2 Scripts do Gerador

### `src/datagen/generator.py`
**O que faz:** Encapsula a l√≥gica de "inventar" dados.
**Regras de Neg√≥cio Simuladas:**
*   `generate_customers`: Cria perfis com data de cadastro (signup_date) nos √∫ltimos 2 anos.
*   `generate_transactions`: Cria compras associadas a esses clientes. O valor (amount) segue uma distribui√ß√£o log-normal para simular compras reais (muitas compras pequenas, poucas compras grandes).

```python
# -------------------------
# Imports de bibliotecas
# -------------------------

# Pandas para cria√ß√£o e manipula√ß√£o de DataFrames
import pandas as pd

# NumPy para gera√ß√£o de n√∫meros aleat√≥rios e distribui√ß√µes estat√≠sticas
import numpy as np

# Faker para gera√ß√£o de dados sint√©ticos realistas (nomes, e-mails, datas, etc.)
from faker import Faker

# Logger estruturado para padroniza√ß√£o dos logs
from src.lib.logger import StructuredLogger


# Instancia um logger espec√≠fico para o gerador de dados
logger = StructuredLogger("DataGenerator")

# Instancia o Faker para gera√ß√£o de dados falsos
fake = Faker()


class DataGenerator:
    """
    Classe respons√°vel por gerar dados sint√©ticos de clientes e transa√ß√µes.

    Objetivo:
    - Simular dados realistas para testes, aulas e pipelines de dados
    - Garantir reprodutibilidade atrav√©s de seeds fixas
    """

    def __init__(self, seed=42):
        """
        Inicializa o gerador com seed fixa para reprodutibilidade.

        Par√¢metros:
        - seed: valor utilizado para inicializar o gerador aleat√≥rio
        """
        # Define seed do Faker
        Faker.seed(seed)

        # Define seed do NumPy
        np.random.seed(seed)

    def generate_customers(self, count=1000):
        """
        Gera um conjunto de clientes sint√©ticos.

        Campos gerados:
        - customer_id : identificador √∫nico do cliente
        - name        : nome completo
        - email       : e-mail
        - signup_date : data de cadastro (at√© 2 anos atr√°s)
        - region      : regi√£o geogr√°fica
        - segment     : segmento do cliente (com distribui√ß√£o controlada)

        Par√¢metros:
        - count: n√∫mero de clientes a serem gerados

        Retorno:
        - DataFrame Pandas com os clientes gerados
        """
        
        # Log do in√≠cio da gera√ß√£o
        logger.info(f"Generating {count} customers")
        
        data = []

        # Gera registros individuais de clientes
        for _ in range(count):
            data.append({
                # UUID √∫nico para cada cliente
                "customer_id": fake.uuid4(),

                # Nome completo aleat√≥rio
                "name": fake.name(),

                # E-mail aleat√≥rio
                "email": fake.email(),

                # Data de cadastro entre 2 anos atr√°s e hoje
                "signup_date": fake.date_between(
                    start_date='-2y',
                    end_date='today'
                ).isoformat(),

                # Regi√£o com escolha uniforme
                "region": np.random.choice(
                    ["North", "South", "East", "West"]
                ),

                # Segmento com probabilidades definidas
                # Premium 10%, Standard 60%, Basic 30%
                "segment": np.random.choice(
                    ["Premium", "Standard", "Basic"],
                    p=[0.1, 0.6, 0.3]
                )
            })

        # Converte a lista de dicion√°rios em DataFrame
        return pd.DataFrame(data)

    def generate_transactions(self, customer_ids, count=10000):
        """
        Gera transa√ß√µes sint√©ticas associadas a clientes existentes.

        Campos gerados:
        - transaction_id : identificador √∫nico da transa√ß√£o
        - customer_id    : refer√™ncia a um cliente existente
        - date           : data/hora da transa√ß√£o
        - amount         : valor monet√°rio da transa√ß√£o
        - category       : categoria do produto

        Par√¢metros:
        - customer_ids: lista de IDs de clientes v√°lidos
        - count: n√∫mero de transa√ß√µes a serem geradas

        Retorno:
        - DataFrame Pandas com as transa√ß√µes geradas
        """
        
        # Log do in√≠cio da gera√ß√£o
        logger.info(
            f"Generating {count} transactions for {len(customer_ids)} customers"
        )
        
        data = []

        # Converte lista de IDs para array NumPy (melhor performance)
        cust_array = np.array(customer_ids)

        # Gera registros individuais de transa√ß√µes
        for _ in range(count):
            data.append({
                # UUID √∫nico para cada transa√ß√£o
                "transaction_id": fake.uuid4(),

                # Associa a transa√ß√£o a um cliente existente
                "customer_id": np.random.choice(cust_array),

                # Data/hora da transa√ß√£o (at√© 2 anos atr√°s)
                "date": fake.date_time_between(
                    start_date='-2y',
                    end_date='now'
                ).isoformat(),

                # Valor da transa√ß√£o com distribui√ß√£o log-normal
                # Simula comportamento realista de gastos
                "amount": round(
                    np.random.lognormal(mean=3, sigma=1),
                    2
                ),

                # Categoria fict√≠cia do produto (renomeado para manter consist√™ncia)
                "product_id": fake.word()
            })

        # Converte a lista de dicion√°rios em DataFrame
        return pd.DataFrame(data)

```

### `src/datagen/main.py`
**O que faz:** √â a interface de linha de comando (CLI). Orquestra a chamada das fun√ß√µes acima e salva os resultados em disco.
**Bibliotecas:** `click` facilita criar comandos como `--rows 1000`.

```python
# -------------------------
# Imports de bibliotecas
# -------------------------

# Click √© utilizado para criar interfaces de linha de comando (CLI)
import click

# Biblioteca padr√£o para manipula√ß√£o de arquivos e diret√≥rios
import os

# Classe respons√°vel pela gera√ß√£o de dados sint√©ticos
from src.datagen.generator import DataGenerator

# Logger estruturado para padroniza√ß√£o dos logs
from src.lib.logger import StructuredLogger


# Instancia um logger espec√≠fico para o CLI de gera√ß√£o de dados
logger = StructuredLogger("DataGenCLI")


# -------------------------
# Defini√ß√£o do comando CLI
# -------------------------
# Este decorador transforma a fun√ß√£o main em um comando execut√°vel via terminal
@click.command()

# Op√ß√£o para definir a quantidade de registros de transa√ß√µes a serem gerados
@click.option(
    '--rows',
    default=1000,
    help='Number of rows to generate.'
)

# Op√ß√£o para definir o diret√≥rio de sa√≠da dos arquivos gerados
@click.option(
    '--output',
    default='data/bronze',
    help='Output directory.'
)
def main(rows, output):
    """
    CLI respons√°vel pela gera√ß√£o de dados sint√©ticos de clientes e transa√ß√µes.

    Uso t√≠pico:
    python datagen.py --rows 5000 --output data/bronze

    Par√¢metros:
    - rows: n√∫mero de registros de transa√ß√µes
    - output: diret√≥rio onde os arquivos Parquet ser√£o salvos
    """
    
    # Log de in√≠cio da gera√ß√£o de dados com par√¢metros informados
    logger.info(
        "Starting Data Generation",
        rows=rows,
        output=output
    )
    
    # Garante que o diret√≥rio de sa√≠da exista
    os.makedirs(output, exist_ok=True)
    
    # Instancia o gerador de dados sint√©ticos
    gen = DataGenerator()
    
    # -------------------------
    # Gera√ß√£o de dados de Customers
    # -------------------------
    
    # Define a quantidade de clientes como 10% do total de transa√ß√µes
    # Garante pelo menos 1 cliente
    n_customers = max(1, rows // 10)
    
    # Gera o DataFrame de clientes
    df_customers = gen.generate_customers(
        count=n_customers
    )
    
    # Caminho de sa√≠da do arquivo de clientes
    cust_path = os.path.join(
        output,
        "customers.parquet"
    )
    
    # Salva os dados no formato Parquet
    # Em um cen√°rio real, poderia ser JSON ou CSV como dado "raw"
    # Aqui usamos Parquet por efici√™ncia e simplicidade
    df_customers.to_parquet(
        cust_path,
        index=False
    )
    
    # Log de sucesso da gera√ß√£o de clientes
    logger.info(
        f"Saved {len(df_customers)} customers to {cust_path}"
    )
    
    # -------------------------
    # Gera√ß√£o de dados de Transactions
    # -------------------------
    
    # Extrai a lista de IDs de clientes gerados
    customer_ids = df_customers['customer_id'].tolist()
    
    # Gera o DataFrame de transa√ß√µes associadas aos clientes
    df_transactions = gen.generate_transactions(
        customer_ids,
        count=rows
    )
    
    # Caminho de sa√≠da do arquivo de transa√ß√µes
    trans_path = os.path.join(
        output,
        "transactions.parquet"
    )
    
    # Salva os dados de transa√ß√µes em Parquet
    df_transactions.to_parquet(
        trans_path,
        index=False
    )
    
    # Log de sucesso da gera√ß√£o de transa√ß√µes
    logger.info(
        f"Saved {len(df_transactions)} transactions to {trans_path}"
    )


# -------------------------
# Ponto de entrada do script
# -------------------------
if __name__ == '__main__':
    main()

```

---

### Prepara√ß√£o para Versionamento
Para que o `bumpversion` funcione, ele precisa encontrar a string de vers√£o no arquivo inicial.
Edite o arquivo `src/lib/__init__.py` e adicione a seguinte linha:

```python
__version__ = "0.1.0"
```

### üõë Checkpoint: Execu√ß√£o & Versionamento (v0.2.0)

Agora que foi criado o Gerador de Dados, este deve ser executado, o progresso salvo e a vers√£o 0.2.0 lan√ßada.


1.  **Executar o Gerador:**
    ```bash
    make dataset
    # ou: poetry run python src/datagen/main.py --rows 1000000
    ```
**Os arquivos ser√£o gerados em:**
- `data/bronze/customers.parquet`
- `data/bronze/transactions.parquet` 

2.  **Commitar Mudan√ßas:**
    ```bash
    git add .
    git commit -m "feat: implement data generator"
    ```

3.  **Lan√ßar Vers√£o v0.2.0:**
    ```bash
    make bump-minor
    # Isso atualizar√° automaticamente a vers√£o de 0.1.0 para 0.2.0
    ```

4.  **Sincronizar com Reposit√≥rio Remoto:**
    ```bash
    git push origin main
    git push --tags
    ```
---

# Cap√≠tulo 3: Ingest√£o Bronze (Fase 3)

## 3.1 Vis√£o Geral
Os arquivos `parquet` (que poderiam ser CSV, JSON) s√£o transformados em formato **Delta Lake**.
**Por que Delta?** Parquet comum n√£o suporta atualiza√ß√µes at√¥micas (ACID). Se um job falhar no meio, dados podem ficar corrompidos. O Delta resolve isso.
**Data Contracts:** Tamb√©m √© introduzida aqui uma valida√ß√£o b√°sica. Se o arquivo n√£o tiver a coluna `customer_id`, a ingest√£o √© rejeitada.

## 3.2 Valida√ß√£o e Jobs

### `src/lib/quality.py`
**O que faz:** Verifica se o DataFrame atende aos requisitos m√≠nimos antes de salvar.

```python
# -------------------------
# Imports do PySpark
# -------------------------

# DataFrame √© a estrutura b√°sica de dados no Spark
from pyspark.sql import DataFrame

# Fun√ß√µes auxiliares para opera√ß√µes em colunas (n√£o usadas aqui, mas comuns em valida√ß√µes)
from pyspark.sql.functions import col

# Logger estruturado para padroniza√ß√£o dos logs
from src.lib.logger import StructuredLogger


# Instancia um logger espec√≠fico para valida√ß√µes de qualidade
logger = StructuredLogger("DataQuality")


class DataQuality:
    """
    Classe respons√°vel por validar contratos m√≠nimos de qualidade de dados
    antes que os dados avancem no pipeline (ex: Bronze -> Silver).
    
    Esta implementa√ß√£o √© propositalmente simples e did√°tica.
    
    > **Nota para Produ√ß√£o:**
    > Esta classe n√£o valida ranges, tipos complexos ou nulidade avan√ßada.
    > Em ambientes reais, ela deve ser substitu√≠da por frameworks robustos como:
    > - Great Expectations
    > - AWS Deequ
    > - Soda
    """

    def __init__(self, context_name: str):
        """
        Inicializa o contexto da valida√ß√£o.

        Par√¢metros:
        - context_name: nome l√≥gico da tabela ou etapa do pipeline
        """
        self.context = context_name

    def validate_contract(self, df: DataFrame, required_columns: list) -> bool:
        """
        Valida√ß√£o b√°sica de contrato de dados.

        Regras implementadas:
        1. Verificar se todas as colunas obrigat√≥rias existem
        2. Verificar se o DataFrame n√£o est√° vazio

        Observa√ß√£o:
        - Esta valida√ß√£o N√ÉO verifica tipos, ranges ou regras estat√≠sticas
        - Serve como uma prote√ß√£o m√≠nima contra dados quebrados

        Par√¢metros:
        - df: DataFrame Spark a ser validado
        - required_columns: lista de colunas obrigat√≥rias

        Retorno:
        - True  -> contrato atendido
        - False -> contrato violado
        """
        
        # Log de in√≠cio da valida√ß√£o com contexto e colunas exigidas
        logger.info(
            f"Validating contract for {self.context}",
            required_columns=required_columns
        )
        
        # -------------------------
        # 1. Valida√ß√£o de Schema (exist√™ncia das colunas)
        # -------------------------
        
        # Conjunto de colunas existentes no DataFrame
        df_cols = set(df.columns)
        
        # Identifica colunas obrigat√≥rias ausentes
        missing_cols = [
            c for c in required_columns
            if c not in df_cols
        ]
        
        # Se houver colunas ausentes, o contrato falha
        if missing_cols:
            logger.error(
                "Contract Failed: Missing columns",
                missing=missing_cols
            )
            return False
            
        # -------------------------
        # 2. Valida√ß√£o de DataFrame vazio
        # -------------------------
        # Verifica se o DataFrame n√£o possui nenhuma linha
        # (checagem simples e opcional, mas recomendada)
        if df.rdd.isEmpty():
            logger.error(
                "Contract Failed: DataFrame is empty"
            )
            return False
            
        # Se todas as valida√ß√µes passaram
        logger.info("Contract Validation Passed")
        return True

```

### `src/jobs/ingestion_bronze.py`
**O que faz:**
1. L√™ os arquivos brutos (Raw).
2. Valida qualidade.
3. Escreve na camada Bronze como tabela Delta.
**Nota:** Usa-se `.option("overwriteSchema", "true")` para permitir que o esquema evolua caso sejam adicionadas colunas no futuro.

```python
# -------------------------
# Imports do PySpark
# -------------------------

# SparkSession √© o ponto de entrada para aplica√ß√µes Spark
from pyspark.sql import SparkSession

# Tipos de dados e defini√ß√£o expl√≠cita de schema
from pyspark.sql.types import (
    StructType,
    StructField,
    StringType,
    IntegerType,
    DoubleType,
    TimestampType
)

# Biblioteca padr√£o para manipula√ß√£o de arquivos e caminhos
import os

# Utilit√°rio para cria√ß√£o da SparkSession com configura√ß√µes padronizadas
from src.lib.spark_utils import get_spark_session

# Logger estruturado para padronizar logs (n√≠vel, mensagem, contexto)
from src.lib.logger import StructuredLogger

# Classe respons√°vel por valida√ß√µes de qualidade de dados (Data Contracts)
from src.lib.quality import DataQuality


# Instancia um logger espec√≠fico para a camada Bronze
logger = StructuredLogger("IngestionBronze")


def ingest_table(
    spark: SparkSession,
    table_name: str,
    source_path: str,
    schema: StructType,
    required_columns: list
):
    """
    Fun√ß√£o gen√©rica de ingest√£o para a camada Bronze.

    Responsabilidades:
    1. Ler dados brutos (Parquet) com schema expl√≠cito
    2. Validar contrato m√≠nimo de qualidade
    3. Persistir os dados no formato Delta Lake (camada Bronze)

    Par√¢metros:
    - spark: SparkSession ativa
    - table_name: nome l√≥gico da tabela
    - source_path: caminho do arquivo de origem
    - schema: schema esperado dos dados
    - required_columns: lista de colunas obrigat√≥rias (contrato m√≠nimo)
    """
    
    # Log de in√≠cio da ingest√£o
    logger.info(
        f"Starting ingestion for {table_name}",
        source=source_path
    )
    
    # -------------------------
    # Valida√ß√£o de exist√™ncia do arquivo de origem
    # -------------------------
    if not os.path.exists(source_path):
        # Se o arquivo n√£o existir, registra warning e interrompe a ingest√£o
        logger.warn(
            f"Source file not found: {source_path}. Skipping."
        )
        return

    # -------------------------
    # 1. Leitura dos dados brutos
    # -------------------------
    # Leitura expl√≠cita com schema definido para evitar infer√™ncia incorreta
    df_raw = (
        spark.read
        .format("parquet")
        .schema(schema)
        .load(source_path)
    )
    
    # -------------------------
    # 2. Valida√ß√£o de Qualidade (Data Contract)
    # -------------------------
    # Instancia o validador de qualidade para a tabela
    validator = DataQuality(table_name)
    
    # Verifica se as colunas obrigat√≥rias est√£o presentes e v√°lidas
    if not validator.validate_contract(
        df_raw,
        required_columns=required_columns
    ):
        # Interrompe o pipeline se o contrato m√≠nimo falhar
        raise ValueError(
            f"Data Quality Contract failed for {table_name}"
        )

    # -------------------------
    # 3. Escrita na camada Bronze (Delta Lake)
    # -------------------------
    # Define o caminho de destino da tabela Bronze
    dest_path = f"data/bronze/{table_name}"
    
    # Observa√ß√£o:
    # - A camada Bronze normalmente √© append-only ou snapshot
    # - Aqui utilizamos overwrite para simplifica√ß√£o do pipeline local (batch did√°tico)
    # - Em produ√ß√£o: Bronze=Append, Silver=Merge, Gold=Overwrite/Merge
    (
        df_raw.write
        .format("delta")
        .mode("overwrite")
        .option("overwriteSchema", "true")
        .save(dest_path)
    )
        
    # Log de sucesso da ingest√£o
    logger.info(
        f"Successfully ingested {table_name} to {dest_path}"
    )


def main():
    """
    Orquestra a ingest√£o dos datasets brutos para a camada Bronze.
    
    Tabelas processadas:
    - customers
    - transactions
    """
    
    # Cria a SparkSession
    spark = get_spark_session("IngestionBronze")
    
    # -------------------------
    # Ingest√£o da tabela Customers
    # -------------------------
    
    # Defini√ß√£o expl√≠cita do schema da tabela customers
    cust_schema = StructType([
        StructField("customer_id", StringType(), False),
        StructField("name", StringType(), True),
        StructField("email", StringType(), True),
        # Na camada Bronze geralmente mantemos tipos crus (string)
        StructField("signup_date", StringType(), True),
        StructField("region", StringType(), True)
    ])
    
    # Chamada da fun√ß√£o gen√©rica de ingest√£o
    ingest_table(
        spark,
        "customers",
        "data/bronze/customers.parquet",
        cust_schema,
        required_columns=["customer_id"]
    )
    
    # -------------------------
    # Ingest√£o da tabela Transactions
    # -------------------------
    
    # Defini√ß√£o expl√≠cita do schema da tabela transactions
    trans_schema = StructType([
        StructField("transaction_id", StringType(), False),
        StructField("customer_id", StringType(), False),
        StructField("product_id", StringType(), True),
        StructField("amount", DoubleType(), True),
        # Mantido como string na Bronze para convers√£o posterior
        StructField("date", StringType(), True)
    ])
    
    # Chamada da fun√ß√£o gen√©rica de ingest√£o
    ingest_table(
        spark,
        "transactions",
        "data/bronze/transactions.parquet",
        trans_schema,
        required_columns=["transaction_id", "customer_id", "amount"]
    )
    
    # Log de finaliza√ß√£o do pipeline Bronze
    logger.info("Bronze Ingestion Complete")
    
    # Encerra a SparkSession
    spark.stop()


# Ponto de entrada do script
if __name__ == "__main__":
    main()

```

### üõë Checkpoint: Execu√ß√£o & Versionamento (v0.3.0)

Os dados brutos ser√£o ingeridos e o contrato validado.

1.  **Executar Ingest√£o Bronze:**

    ```bash
    poetry run python src/jobs/ingestion_bronze.py
    ```
    *Verifique se a pasta `data/bronze/customers` (Delta) foi criada.*

- Os avisos (WARN) que aparecer√£o durante a execu√ß√£o, s√£o apenas o Spark reclamando que est√° rodando localmente (loopback IP) e sem bibliotecas nativas do Hadoop otimizadas para o seu SO, o que √© completamente normal e esperado para um ambiente de desenvolvimento local. 
- OBS: Quando as pessoas dizem "Hadoop est√° obsoleto", elas geralmente se referem a duas coisas:

    - HDFS (Sistema de Arquivos): Foi amplamente substitu√≠do pelo armazenamento em nuvem (S3, Azure Blob, GCS).
    - MapReduce (Motor de Processamento): Foi totalmente esmagado pelo Spark. O Spark √© 100x mais r√°pido porque processa em mem√≥ria (RAM), enquanto o   - MapReduce gravava em disco a cada etapa.

- Ent√£o por que o Spark ainda pede "bibliotecas do Hadoop"? Porque, ironicamente, o Spark nasceu dentro do ecossistema Hadoop. Mesmo que voc√™ use S3 e Spark, o Spark ainda usa internamente as APIs de cliente do Hadoop (as hadoop-client-libs) para saber como "falar" com sistemas de arquivos distribu√≠dos. Ele usa o c√≥digo legado do Hadoop apenas como um "driver" ou "conector" para ler/gravar arquivos, n√£o para processar os dados.

Resumindo: O Hadoop como plataforma central (clusters gigantes de HDFS + YARN) est√° de fato em decl√≠nio/legado, mas o c√≥digo do cliente Hadoop ainda vive escondido dentro do Spark como uma depend√™ncia de baixo n√≠vel para I/O.



2.  **Commitar Mudan√ßas:**
    ```bash
    git add .
    git commit -m "feat: implement bronze ingestion"
    ```

3.  **Lan√ßar Vers√£o v0.3.0:**
    ```bash
    make bump-minor
    # Isso atualizar√° automaticamente a vers√£o de 0.2.0 para 0.3.0
    ```

4.  **Sincronizar com Reposit√≥rio Remoto:**
    ```bash
    git push origin main
    git push --tags
    ```
---

# Cap√≠tulo 4: Refinamento Silver (Fase 4)

## 4.1 Vis√£o Geral
A camada **Silver** √© o cora√ß√£o da qualidade do Data Lakehouse. Enquanto a Bronze √© uma c√≥pia fiel (e muitas vezes "suja") da origem, a Silver cont√©m dados limpos, tipados e confi√°veis, prontos para an√°lise explorat√≥ria.

Nesta etapa, √© essencial aplicar regras rigorosas de transforma√ß√£o:

1.  **Deduplica√ß√£o (De-duplication):**
    Garante a unicidade dos registros (idempot√™ncia). Se o sistema de origem enviou a mesma transa√ß√£o duas vezes por erro de rede ou reprocessamento, o Spark detecta e mant√©m apenas uma vers√£o, eliminando redund√¢ncias.

2.  **Imposi√ß√£o de Schema e Tipagem (Schema Enforcement):**
    Converte dados gen√©ricos (onde tudo chega como `string`) para tipos espec√≠ficos (`Integer`, `Double`, `Date`, `Timestamp`). Isso habilita opera√ß√µes aritm√©ticas e filtros de data perform√°ticos, al√©m de evitar erros de convers√£o no futuro.
    *   Exemplo: `"2023-01-01"` (String) ‚Üí `2023-01-01` (DateType).

3.  **Padroniza√ß√£o e Renomea√ß√£o (Standardization):**
    Ajusta nomes de colunas para seguir conven√ß√µes de engenharia (snake_case) e evita conflitos com palavras reservadas de SQL/Spark.
    *   Exemplo: Alterar a coluna `date` para `transaction_date`, pois `DATE` √© um tipo de dado reservado em SQL.

### `src/jobs/refine_silver.py`

```python
# Importa a classe principal do Spark para cria√ß√£o da SparkSession
from pyspark.sql import SparkSession

# Importa fun√ß√µes auxiliares do Spark SQL para manipula√ß√£o de colunas e datas
from pyspark.sql.functions import col, to_date

# Fun√ß√£o utilit√°ria que cria uma SparkSession j√° configurada (Delta Lake, modo local, etc.)
from src.lib.spark_utils import get_spark_session

# Logger estruturado em JSON para padronizar logs do projeto
from src.lib.logger import StructuredLogger


# Instancia um logger espec√≠fico para esta etapa do pipeline (Silver)
logger = StructuredLogger("RefineSilver")


def refine_customers(spark: SparkSession):
    """
    Refina a tabela de clientes da camada Bronze para a camada Silver.
    
    Responsabilidades:
    - Ler dados brutos (Delta Bronze)
    - Remover duplicidades
    - Converter tipos de dados
    - Persistir dados limpos como Delta Lake (Silver)
    """
    
    # Caminho de origem (Bronze) e destino (Silver)
    source_path = "data/bronze/customers"
    dest_path = "data/silver/customers"
    
    # Log de in√≠cio do processamento
    logger.info("Refining customers", source=source_path)
    
    # Leitura da tabela Delta da camada Bronze
    df = spark.read.format("delta").load(source_path)
    
    # -------------------------
    # Deduplica√ß√£o
    # -------------------------
    # Conta registros antes da deduplica√ß√£o (a√ß√£o Spark)
    # ‚ö†Ô∏è Performance Warning: count() √© custoso em Big Data.
    # Em produ√ß√£o real, prefira m√©tricas aproximadas ou Delta History.
    initial_count = df.count()
    
    # Remove registros duplicados com base na chave de neg√≥cio customer_id
    df_dedup = df.dropDuplicates(["customer_id"])
    
    # Conta registros ap√≥s a deduplica√ß√£o
    final_count = df_dedup.count()
    
    # Caso existam duplicatas, registra no log a quantidade removida
    if initial_count != final_count:
        logger.info(f"Removed {initial_count - final_count} duplicates from customers")
        
    # -------------------------
    # Transforma√ß√µes (Bronze -> Silver)
    # -------------------------
    # Converte a coluna signup_date de string para DateType
    # Assume-se formato padr√£o yyyy-MM-dd
    # Caso o formato fosse inv√°lido, o problema deveria ser tratado na valida√ß√£o
    df_silver = df_dedup.withColumn(
        "signup_date",
        to_date(col("signup_date"))
    )
    
    # -------------------------
    # Escrita na camada Silver
    # -------------------------
    # Salva os dados refinados como tabela Delta
    # overwrite: reprocessamento idempotente
    # overwriteSchema: permite evolu√ß√£o de schema
    df_silver.write \
        .format("delta") \
        .mode("overwrite") \
        .option("overwriteSchema", "true") \
        .save(dest_path)
        
    # Log de sucesso
    logger.info(f"Saved customers Silver at {dest_path}")


def refine_transactions(spark: SparkSession):
    """
    Refina a tabela de transa√ß√µes da camada Bronze para a camada Silver.
    
    Responsabilidades:
    - Ler dados brutos
    - Remover duplicidades
    - Renomear e tipar colunas
    - Persistir dados confi√°veis para an√°lises
    """
    
    # Caminho de origem (Bronze) e destino (Silver)
    source_path = "data/bronze/transactions"
    dest_path = "data/silver/transactions"
    
    # Log de in√≠cio do processamento
    logger.info("Refining transactions", source=source_path)
    
    # Leitura da tabela Delta da camada Bronze
    df = spark.read.format("delta").load(source_path)
    
    # -------------------------
    # Deduplica√ß√£o
    # -------------------------
    # Remove duplicidades com base no identificador √∫nico da transa√ß√£o
    df_dedup = df.dropDuplicates(["transaction_id"])
    
    # -------------------------
    # Transforma√ß√µes
    # -------------------------
    # Renomeia a coluna 'date' para 'transaction_date'
    # Evita conflito com palavra reservada SQL e melhora sem√¢ntica
    # Converte de string/timestamp ISO para DateType
    #
    # Exemplo de valor original:
    # 2024-01-01T10:00:00
    # Ap√≥s to_date -> 2024-01-01
    df_silver = df_dedup \
        .withColumnRenamed("date", "transaction_date") \
        .withColumn(
            "transaction_date",
            to_date(col("transaction_date"))
        )
    
    # -------------------------
    # Escrita na camada Silver
    # -------------------------
    # Salva como Delta Lake, garantindo idempot√™ncia e schema evolutivo
    df_silver.write \
        .format("delta") \
        .mode("overwrite") \
        .option("overwriteSchema", "true") \
        .save(dest_path)
        
    # Log de sucesso
    logger.info(f"Saved transactions Silver at {dest_path}")


def main():
    """
    Fun√ß√£o principal do job Spark.
    Orquestra o refinamento das tabelas de clientes e transa√ß√µes.
    """
    
    # Cria a SparkSession configurada para o projeto
    spark = get_spark_session("RefineSilver")
    
    # Executa o refinamento de cada entidade
    refine_customers(spark)
    refine_transactions(spark)
    
    # Log de conclus√£o do job
    logger.info("Silver Refinement Complete")
    
    # Encerra a SparkSession de forma limpa
    spark.stop()


# Ponto de entrada do script
if __name__ == "__main__":
    main()

```

### üõë Checkpoint: Execu√ß√£o & Versionamento (v0.4.0)

Momento de limpar os dados e criar a camada confi√°vel.

1.  **Executar Refinamento Silver:**
    ```bash
    poetry run python src/jobs/refine_silver.py
    ```
    *Verifique se a pasta `data/silver/` foi populada.*

2.  **Commitar Mudan√ßas:**
    ```bash
    git add .
    git commit -m "feat: implement silver refinement"
    ```

3.  **Lan√ßar Vers√£o v0.4.0:**
    ```bash
    make bump-minor
    # Isso atualizar√° automaticamente a vers√£o de 0.3.0 para 0.4.0
    ```

4.  **Sincronizar com Reposit√≥rio Remoto:**
    ```bash
    git push origin main
    git push --tags
    ```

---

# Cap√≠tulo 5: Agrega√ß√£o Gold (Fase 5)

## 5.1 Vis√£o Geral: Feature Store
A camada **Gold** √© onde os dados se transformam em **Intelig√™ncia**. Aqui, deixamos de olhar para "transa√ß√µes" (eventos isolados) e passamos a olhar para "entidades" (o Cliente).

Machine Learning n√£o aprende bem com dados brutos como "Jo√£o comprou p√£o na ter√ßa" e "Jo√£o comprou leite na quarta". Os modelos precisam de **Features** (Atributos) consolidados que descrevam o perfil de consumo.

Ser√° utilizada a metodologia **RFM**, um cl√°ssico do Marketing Anal√≠tico:

1.  **Rec√™ncia (Recency):** "H√° quanto tempo o cliente n√£o compra?"
    *   *Insight:* Clientes que compraram recentemente s√£o mais propensos a comprar de novo do que aqueles que sumiram h√° 6 meses.

2.  **Frequ√™ncia (Frequency):** "Quantas vezes ele comprou no total?"
    *   *Insight:* Clientes fi√©is t√™m alta frequ√™ncia. Clientes espor√°dicos t√™m baixa.

3.  **Monet√°rio (Monetary):** "Quanto ele gasta em m√©dia (Ticket M√©dio)?"
    *   *Insight:* Separa os clientes "Baleia" (alto valor) dos clientes "Sardinha" (baixo valor).

O resultado final ser√° uma **Tabela Anal√≠tica (ABT - Analytical Base Table)** com uma linha √∫nica por cliente, pronta para alimentar modelos de Churn e LTV.

### `src/jobs/agg_gold.py`
**Destaque T√©cnico (Left Join Strategy):**
A intui√ß√£o inicial seria fazer um `inner join` entre Clientes e Transa√ß√µes. Por√©m, isso seria um **erro grave** para modelagem.
*   **O Problema:** Um `inner join` descartaria todos os clientes que se cadastraram mas nunca compraram.
*   **A Solu√ß√£o:** Use `left join` (Cliente -> Transa√ß√µes).
*   **Por que isso importa?** O modelo de Machine Learning precisa aprender o padr√£o de **quem compra** E TAMB√âM o padr√£o de **quem N√ÉO compra**. Clientes com `frequency = 0` s√£o exemplos valiosos de "inativos" ou "churners imediatos". Se voc√™ os remove, introduz um **Vi√©s de Sobreviv√™ncia (Survivorship Bias)** nos dados.

```python
# Importa a classe SparkSession (usada apenas para tipagem/documenta√ß√£o)
from pyspark.sql import SparkSession

# Importa fun√ß√µes do Spark SQL com alias padr√£o de mercado
from pyspark.sql import functions as F

# Fun√ß√£o utilit√°ria para criar SparkSession configurada com Delta Lake
from src.lib.spark_utils import get_spark_session

# Logger estruturado em JSON
from src.lib.logger import StructuredLogger


# Logger espec√≠fico para a camada Gold
logger = StructuredLogger("AggGold")


def main():
    """
    Job respons√°vel pela cria√ß√£o da camada Gold (Feature Store).

    Objetivos:
    - Consolidar dados transacionais em n√≠vel de cliente
    - Gerar features RFM
    - Produzir dados prontos para Analytics e Machine Learning
    """

    # Inicializa Spark
    spark = get_spark_session("AggGold")

    # -------------------------
    # Defini√ß√£o dos caminhos
    # -------------------------
    customers_path = "data/silver/customers"
    transactions_path = "data/silver/transactions"
    gold_path = "data/gold/customer_features"

    logger.info("Starting Gold Aggregation (RFM Features)")

    # -------------------------
    # 1. Leitura dos dados Silver
    # -------------------------
    df_cust = spark.read.format("delta").load(customers_path)
    df_trans = spark.read.format("delta").load(transactions_path)

    # -------------------------
    # 2. Data de refer√™ncia
    # -------------------------
    # Em produ√ß√£o poderia ser current_date()
    # Aqui usamos a maior data dispon√≠vel no dataset
    max_date = (
        df_trans
        .select(F.max("transaction_date"))
        .collect()[0][0]
    )

    logger.info(f"Reference Date for Recency: {max_date}")

    # -------------------------
    # 3. Agrega√ß√£o RFM
    # -------------------------
    df_rfm = df_trans.groupBy("customer_id").agg(
        F.max("transaction_date").alias("last_purchase_date"),
        F.count("transaction_id").alias("frequency"),
        F.avg("amount").alias("monetary_value")
    )

    # -------------------------
    # 4. C√°lculo da Recency
    # -------------------------
    # Recency = dias desde a √∫ltima compra
    df_rfm = df_rfm.withColumn(
        "recency",
        F.datediff(
            F.lit(max_date),
            F.col("last_purchase_date")
        )
    )

    # -------------------------
    # 5. Join com perfil do cliente
    # -------------------------
    # LEFT JOIN para manter clientes sem transa√ß√µes
    df_final = df_cust.join(
        df_rfm,
        on="customer_id",
        how="left"
    )

    # -------------------------
    # 6. Tratamento correto de clientes sem compras (OP√á√ÉO A)
    # -------------------------
    # Para clientes sem compras:
    # - frequency = 0
    # - monetary_value = 0
    # - recency = dias desde o signup at√© a data de refer√™ncia
    #
    # Isso evita valores sentinela artificiais (ex: 999)
    # e mant√©m coer√™ncia temporal e estat√≠stica
    df_final = (
        df_final
        .withColumn(
            "recency",
            F.when(
                F.col("recency").isNull(),
                F.datediff(
                    F.lit(max_date),
                    F.col("signup_date")
                )
            ).otherwise(F.col("recency"))
        )
        .fillna({
            "frequency": 0,
            "monetary_value": 0
        })
    )

    # -------------------------
    # 7. Escrita da camada Gold
    # -------------------------
    logger.info(f"Saving features to {gold_path}")

    df_final.write \
        .format("delta") \
        .mode("overwrite") \
        .option("overwriteSchema", "true") \
        .save(gold_path)

    logger.info("Gold Aggregation Complete")

    # Finaliza Spark
    spark.stop()


# Ponto de entrada
if __name__ == "__main__":
    main()


```

### üõë Checkpoint: Execu√ß√£o & Versionamento (v0.5.0)

Cria√ß√£o da Feature Store para o Machine Learning.

1.  **Executar Agrega√ß√£o Gold:**
    ```bash
    poetry run python src/jobs/agg_gold.py
    ```
    *Verifique se a pasta `data/gold/customer_features` foi criada.*

2.  **Commitar Mudan√ßas:**
    ```bash
    git add .
    git commit -m "feat: implement gold features aggregation"
    ```

3.  **Lan√ßar Vers√£o v0.5.0:**
    ```bash
    make bump-minor
    # Isso atualizar√° automaticamente a vers√£o de 0.4.0 para 0.5.0
    ```

4.  **Sincronizar com Reposit√≥rio Remoto:**
    ```bash
    git push origin main
    git push --tags
    ```

---

# Cap√≠tulo 6: Machine Learning (Fase 6)

## 6.1 Vis√£o Geral: De Dados para Decis√µes
Chegamos ao √°pice do projeto. Com dados limpos (Silver) e agregados por comportamento (Gold), podemos finalmente responder perguntas estrat√©gicas de neg√≥cio usando IA.

Este projeto aplica uma abordagem h√≠brida de modelagem:

1.  **Churn Prediction (O "Quem Sai"):**
    *   **Tipo:** Classifica√ß√£o Bin√°ria (Supervisionado).
    *   **Pergunta:** "Qual a probabilidade deste cliente abandonar a empresa nos pr√≥ximos 90 dias?"
    *   **A√ß√£o de Neg√≥cio:** Enviar cupom de reten√ß√£o *apenas* para quem tem alta chance de sair, economizando marketing.
    *   **Algoritmo:** XGBoost (Gradient Boosting), escolhido por sua robustez em dados tabulares desbalanceados.

2.  **LTV Prediction (O "Quem Vale a Pena"):**
    *   **Tipo:** Regress√£o Probabil√≠stica (Generativo).
    *   **Pergunta:** "Quanto dinheiro este cliente ainda vai gastar conosco at√© o fim da vida?"
    *   **A√ß√£o de Neg√≥cio:** Identificar clientes VIP (Baleias/Whales) para dar atendimento premium.
    *   **Algoritmo:** Lifetimes (BG/NBD), que modela estatisticamente a "morte" e a "frequ√™ncia" do cliente.

## 6.2 Scripts de Treinamento

### `src/ml/train_churn.py` (XGBoost)
**Estrat√©gia de Modelagem:**

1.  **Defini√ß√£o do Target (O que √© Churn?):**
    *   Em contratos mensais (ex: Netflix), Churn √© quando o usu√°rio cancela. No varejo, n√£o h√° cancelamento expl√≠cito.
    *   *Defini√ß√£o adotada:* Se o cliente n√£o comprar nada por **90 dias** (Rec√™ncia > 90), ele √© considerado Churn.
    *   *Nota:* Este limiar (threshold) deve ser ajustado conforme o neg√≥cio (ex: venda de im√≥veis vs venda de p√£o).

2.  **Algoritmo: XGBoost (eXtreme Gradient Boosting):**
    *   Foi escolhido por ser o "estado da arte" para dados tabulares estruturados.
    *   *Vantagens:* Lida bem com valores nulos, previne overfitting e captura rela√ß√µes n√£o-lineares complexas (ex: clientes que gastam muito pouco ou muito, ambos podem sair por motivos diferentes).

3.  **Experiment Tracking (MLflow):**
    *   Em vez de apenas printar a acur√°cia no terminal, o MLflow atua como uma "caixa preta" do avi√£o. Ele grava:
        *   **Par√¢metros:** Hiperpar√¢metros usados (learning rate, depth).
        *   **M√©tricas:** AUC, Acur√°cia, F1-Score.
        *   **Artefatos:** O pr√≥prio arquivo do modelo (`model.pkl`) para deploy futuro.

```python
import mlflow
import mlflow.xgboost
import xgboost as xgb
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_auc_score

from src.lib.spark_utils import get_spark_session
from src.lib.logger import StructuredLogger


# Logger espec√≠fico para o treinamento de churn
logger = StructuredLogger("TrainChurn")


def main():
    """
    Job de treinamento do modelo de Churn utilizando XGBoost.

    Objetivos:
    - Ler features da camada Gold
    - Criar vari√°vel alvo (churn)
    - Tratar desbalanceamento de classes via scale_pos_weight
    - Treinar e avaliar modelo
    - Registrar experimento no MLflow
    """

    # -------------------------
    # 1. Leitura dos dados (Gold)
    # -------------------------
    spark = get_spark_session("TrainChurn")

    logger.info("Loading Gold data...")
    df_spark = spark.read.format("delta").load(
        "data/gold/customer_features"
    )

    # Converte para Pandas (assume dados cabem em mem√≥ria)
    df = df_spark.toPandas()
    logger.info(f"Loaded {len(df)} records")

    # -------------------------
    # 2. Feature Engineering
    # -------------------------
    # Defini√ß√£o do alvo:
    # Cliente √© considerado churn se n√£o compra h√° mais de 90 dias
    df["churn"] = (df["recency"] > 90).astype(int)

    features = ["frequency", "monetary_value"]
    target = "churn"

    X = df[features]
    y = df[target]

    # -------------------------
    # 3. Split Train / Test
    # -------------------------
    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=0.3,
        random_state=42,
        stratify=y  # mant√©m propor√ß√£o das classes
    )

    # -------------------------
    # 4. C√°lculo do scale_pos_weight
    # -------------------------
    # F√≥rmula recomendada pelo XGBoost:
    # scale_pos_weight = (# negativos) / (# positivos)
    # Convers√£o expl√≠cita para int/float nativos do Python
    # (JSON n√£o serializa tipos do NumPy como int64/float32)
    neg = int((y_train == 0).sum())
    pos = int((y_train == 1).sum())

    scale_pos_weight = neg / pos

    logger.info(
        "Class balance (train set)",
        negative=neg,
        positive=pos,
        scale_pos_weight=scale_pos_weight
    )

    # -------------------------
    # 5. Treinamento do modelo
    # -------------------------
    mlflow.set_experiment("churn_prediction")

    with mlflow.start_run():

        logger.info("Training XGBoost Classifier...")

        model = xgb.XGBClassifier(
            objective="binary:logistic",
            n_estimators=100,
            learning_rate=0.1,
            max_depth=5,
            eval_metric="logloss",
            scale_pos_weight=scale_pos_weight,
            use_label_encoder=False,
            random_state=42
        )

        model.fit(X_train, y_train)

        # -------------------------
        # 6. Avalia√ß√£o
        # -------------------------
        y_pred = model.predict(X_test)
        y_prob = model.predict_proba(X_test)[:, 1]

        acc = accuracy_score(y_test, y_pred)
        auc = roc_auc_score(y_test, y_prob)

        logger.info(f"Accuracy: {acc:.4f}")
        logger.info(f"ROC-AUC: {auc:.4f}")

        # -------------------------
        # 7. Registro no MLflow
        # -------------------------
        mlflow.log_params({
            "n_estimators": 100,
            "learning_rate": 0.1,
            "max_depth": 5,
            "scale_pos_weight": scale_pos_weight
        })

        mlflow.log_metric("accuracy", acc)
        mlflow.log_metric("roc_auc", auc)

        mlflow.xgboost.log_model(
            model,
            artifact_path="model"
        )

        logger.info("Model trained and logged to MLflow successfully")

    # Encerra Spark
    spark.stop()


# Ponto de entrada
if __name__ == "__main__":
    main()


```

### `src/ml/train_ltv.py` (Lifetimes)
**Estrat√©gia de Modelagem:**

1.  **Framework Te√≥rico (BG/NBD):**
    *   Utilizamos o modelo **Beta-Geometric / Negative Binomial Distribution**, popularizado por Peter Fader (Wharton School).
    *   *Como funciona:* Ele assume que o processo de compra do cliente segue uma distribui√ß√£o de probabilidade (moeda viciada). Enquanto a moeda der "cara", ele continua "vivo" e comprando. Se der "coroa", ele "morre" (Churn). O modelo estima os par√¢metros latentes dessas moedas para cada cliente.

2.  **Prepara√ß√£o dos Dados (Lifetimes format):**
    *   O algoritmo exige um formato espec√≠fico:
        *   `frequency`: Quantas compras repetidas ele fez?
        *   `recency`: Qual a "idade" do cliente no momento da √∫ltima compra?
        *   `T`: Qual a idade total do cliente hoje?
    *   *Valida√ß√£o Cr√≠tica:* Em dados reais (e sint√©ticos), √†s vezes ocorrem anomalias onde a data da compra √© anterior ao cadastro. O script aplica um filtro rigoroso (`mask_valid`) para garantir que `T >= recency`, caso contr√°rio o modelo matem√°tico quebra.

3.  **Regulariza√ß√£o (Penalizer):**
    *   Como muitos clientes t√™m poucas compras (esparsidade), o modelo pode "alucinar" e prever que um cliente que comprou 1 vez vai comprar 1000 vezes amanh√£.
    *   *Solu√ß√£o:* Aplicamos `penalizer_coef=0.1` para "acalmar" o modelo e for√ßar previs√µes mais conservadoras e realistas.

```python
# -------------------------
# Imports de MLOps e Modelagem Estat√≠stica
# -------------------------

# MLflow para rastreamento de experimentos (par√¢metros, m√©tricas e execu√ß√µes)
import mlflow

# Pandas para manipula√ß√£o de dados em mem√≥ria
import pandas as pd

# Modelo probabil√≠stico BG/NBD para previs√£o de frequ√™ncia de compras (LTV)
from lifetimes import BetaGeoFitter

# Utilit√°rio para cria√ß√£o da SparkSession configurada (Delta Lake, modo local)
from src.lib.spark_utils import get_spark_session

# Logger estruturado em JSON para padroniza√ß√£o dos logs
from src.lib.logger import StructuredLogger


# Instancia um logger espec√≠fico para o treinamento do modelo de LTV
logger = StructuredLogger("TrainLTV")


def main():
    """
    Script respons√°vel pelo treinamento do modelo de LTV (Lifetime Value),
    utilizando o modelo estat√≠stico BG/NBD (Beta-Geometric / Negative Binomial Distribution).
    
    Pipeline de alto n√≠vel:
    1. Carregar dados da camada Gold (Feature Store)
    2. Preparar os dados no formato exigido pelo Lifetimes
    3. Treinar o modelo BG/NBD
    4. Estimar o n√∫mero esperado de compras futuras
    5. Registrar experimento no MLflow
    """
    
    # Cria a SparkSession para leitura dos dados Delta
    spark = get_spark_session("TrainLTV")
    
    # -------------------------
    # 1. Leitura dos dados (Camada Gold)
    # -------------------------
    logger.info("Loading Gold data for LTV...")
    
    # L√™ a tabela de features consolidadas por cliente
    df_spark = spark.read.format("delta").load(
        "data/gold/customer_features"
    )
    
    # Converte o DataFrame Spark para Pandas
    # OBS: Assume-se que o volume de dados cabe em mem√≥ria
    # Em cen√°rios de grande escala, alternativas incluem:
    # - Spark MLlib
    # - Feature Store externa (Feast, Hopsworks)
    # - Amostragem controlada para treino local
    df = df_spark.toPandas()
    
    # -------------------------
    # 2. Prepara√ß√£o dos dados para o Lifetimes
    # -------------------------
    # O modelo BG/NBD exige tr√™s vari√°veis principais:
    #
    # - frequency: n√∫mero de compras repetidas
    # - recency: "idade" do cliente no momento da √∫ltima compra
    # - T: idade total do cliente no per√≠odo de observa√ß√£o
    #
    # ATEN√á√ÉO:
    # - A coluna 'recency' da camada Gold N√ÉO √© compat√≠vel com o Lifetimes
    # - Precisamos recalcular recency e T a partir das datas
    
    # Converte colunas de data para datetime
    df['signup_date'] = pd.to_datetime(df['signup_date'])
    df['last_purchase_date'] = pd.to_datetime(df['last_purchase_date'])
    
    # Define a data de refer√™ncia do modelo
    # Em dados reais, poderia ser a data atual (today)
    # Aqui usamos a maior data encontrada no dataset
    current_date = df['last_purchase_date'].max()
    
    logger.info(f"Reference Date: {current_date}")
    
    # -------------------------
    # 3. C√°lculo das vari√°veis do Lifetimes
    # -------------------------
    
    # T (Age): idade total do cliente
    # T = data de refer√™ncia - data de cadastro
    df['T'] = (current_date - df['signup_date']).dt.days
    
    # lifetimes_recency: idade do cliente na √∫ltima compra
    # lifetimes_recency = data da √∫ltima compra - data de cadastro
    df['lifetimes_recency'] = (
        df['last_purchase_date'] - df['signup_date']
    ).dt.days
    
    # Para clientes que nunca compraram:
    # - frequency = 0
    # - lifetimes_recency deve ser 0 (exig√™ncia do modelo)
    df.loc[df['frequency'] == 0, 'lifetimes_recency'] = 0
    
    # -------------------------
    # 4. Valida√ß√£o dos dados
    # -------------------------
    # Remove registros inv√°lidos:
    # - recency negativa (compra antes do cadastro)
    # - T negativo (datas inconsistentes)
    mask_valid = (df['lifetimes_recency'] >= 0) & (df['T'] >= 0)
    
    logger.info(
        f"Filtering {len(df) - mask_valid.sum()} invalid records "
        "(negative recency/age)"
    )
    
    df = df[mask_valid]
    
    # DataFrame final usado no treinamento
    data = df[['frequency', 'lifetimes_recency', 'T']].copy()
    
    # -------------------------
    # 5. Treinamento do modelo BG/NBD
    # -------------------------
    # Define o experimento no MLflow
    mlflow.set_experiment("ltv_prediction")
    
    # Inicia um novo run no MLflow
    with mlflow.start_run():
        
        logger.info("Training BetaGeoFitter...")
        
        # Instancia o modelo BG/NBD
        # penalizer_coef atua como regulariza√ß√£o para evitar overfitting
        bgf = BetaGeoFitter(penalizer_coef=0.1)
        
        # Ajusta o modelo aos dados hist√≥ricos
        bgf.fit(
            data['frequency'],
            data['lifetimes_recency'],
            data['T']
        )
        
        # Log do resumo estat√≠stico do modelo
        logger.info(str(bgf))
        
        # -------------------------
        # 6. Previs√£o de compras futuras
        # -------------------------
        # Define horizonte de previs√£o (ex: pr√≥ximos 90 dias)
        t = 90
        
        # Estima o n√∫mero esperado de compras futuras por cliente
        df['predicted_purchases'] = bgf.conditional_expected_number_of_purchases_up_to_time(
            t,
            data['frequency'],
            data['lifetimes_recency'],
            data['T']
        )
        
        # -------------------------
        # 7. An√°lise de resultados
        # -------------------------
        # Exibe os 5 clientes com maior expectativa de compras futuras
        logger.info("Top 5 Customers by Predicted LTV:")
        
        top_5 = (
            df
            .sort_values('predicted_purchases', ascending=False)
            [['customer_id', 'predicted_purchases']]
            .head(5)
        )
        
        logger.info(
            "Top 5 Customers",
            data=top_5.to_dict(orient='records')
        )
        
        # -------------------------
        # 8. Registro no MLflow
        # -------------------------
        # Observa√ß√£o:
        # - O Lifetimes n√£o √© baseado em scikit-learn
        # - O log direto do modelo exige um custom flavor ou pickle
        # - Para simplicidade, registramos apenas par√¢metros
        
        mlflow.log_param("penalizer_coef", 0.0)
        
        logger.info("LTV Model Trained successfully")


# Ponto de entrada do script
if __name__ == "__main__":
    main()

```

### üõë Checkpoint Final: Execu√ß√£o & Versionamento (v0.6.0)

√öltima etapa! Os dois modelos ser√£o treinados e a entrega oficializada.

1.  **Executar Treinamento ML:**
    ```bash
    make train
    # ou: poetry run python src/ml/train_churn.py
    # (O LTV pode ser rodado com: poetry run python src/ml/train_ltv.py)
    ```
    *Verifique os logs de acur√°cia e o output do MLflow.*

    **Resultados Esperados (Exemplo):**
    ```text
    Resultados do Modelo:
    Acur√°cia: 71.90% (0.7190)
    AUC: 53.82% (0.5382)
    ```
    > **Nota:** O AUC est√° baixo (pr√≥ximo de 0.5, que √© aleat√≥rio), o que √© esperado para dados sint√©ticos gerados aleatoriamente. Em dados reais, esperar√≠amos > 0.7.

    **Sobre os Avisos (Warnings):**
    *   **XGBoost:** O aviso sobre `use_label_encoder` √© porque essa funcionalidade foi depreciada nas vers√µes novas. O aviso sobre UBJSON √© apenas informativo.
    *   **MLflow:** O aviso sobre "Model logged without a signature" √© apenas uma sugest√£o para incluir schemas nos metadados, mas n√£o afeta o funcionamento.

2.  **Commitar Mudan√ßas:**
    ```bash
    git add .
    git commit -m "feat: implement ml models"
    ```
- Ao rodar make train, o MLflow criou a pasta mlruns/, e como foi usado git add ., todos esses arquivos de log (m√©tricas, par√¢metros, metadados) foram adicionados ao commit.

- √â normal commitar a pasta mlruns?

- Projetos Pessoais/Estudo: Sim, √© √∫til para manter o hist√≥rico dos seus experimentos junto com o c√≥digo.
- Projetos em Equipe/Produ√ß√£o: Geralmente n√£o. O MLflow seria configurado para salvar esses dados em um servidor remoto (banco de dados + S3) e adicionar√≠amos mlruns/ ao .gitignore.
- Como este √© um projeto standalone de portf√≥lio, n√£o tem problema! 


3.  **Lan√ßar Vers√£o Final (v0.6.0):**
    ```bash
    make bump-minor
    ```

4.  **Sincronizar com Reposit√≥rio Remoto:**
    ```bash
    git push origin main
    git push --tags
    ```

---

# Cap√≠tulo 7: CI/CD & Testes Automatizados (Fase 7)

Para elevar o n√≠vel de profissionalismo do projeto ("Roadmap Enterprise"), foi implementado um pipeline de **Integra√ß√£o Cont√≠nua (CI)**. Isso garante que nenhum c√≥digo quebrado entre na branch `main`.

## 7.0 Configura√ß√£o Inicial
Antes de criar os testes, precisamos preparar a estrutura de diret√≥rios para o ambiente de testes e GitHub Actions.

**A√ß√£o:** Prepare os diret√≥rios no terminal:
```bash
mkdir -p tests .github/workflows
touch tests/__init__.py
```

## 7.1 Smoke Testing (Teste de Fuma√ßa)
Antes de rodar pipelines complexos, precisamos garantir que o Spark consegue iniciar e que as depend√™ncias est√£o corretas.

**A√ß√£o:** Crie o arquivo `tests/test_smoke.py` com o conte√∫do abaixo:
**Objetivo:** Validar se o ambiente Spark/Delta est√° funcional em menos de 10 segundos.

```python
import pytest
from src.lib.spark_utils import get_spark_session

@pytest.fixture(scope="session")
def spark():
    """Shared SparkSession for testing."""
    # Cria uma sess√£o local para testes
    spark = get_spark_session("TestSession", local=True)
    yield spark
    spark.stop()

def test_spark_session_is_active(spark):
    """Smoke test: Validation of Spark Session."""
    data = [("ok", 1)]
    df = spark.createDataFrame(data, ["status", "id"])
    
    # Verifica escrita/leitura em mem√≥ria
    assert df.count() == 1
    assert df.collect()[0]["status"] == "ok"
```

## 7.2 Teste de Integra√ß√£o (L√≥gica de Deduplica√ß√£o)
Al√©m de verificar se o Spark liga, precisamos verificar se a **l√≥gica de neg√≥cio** (ETL) est√° correta.

**A√ß√£o:** Crie o arquivo `tests/test_silver.py` com o conte√∫do abaixo:
**Objetivo:** Garantir que a deduplica√ß√£o da camada Silver realmente funciona.

```python
import pytest
from pyspark.sql import Row
from src.lib.spark_utils import get_spark_session

@pytest.fixture(scope="session")
def spark():
    """Shared SparkSession for testing."""
    spark = get_spark_session("TestSilver", local=True)
    yield spark
    spark.stop()

def test_refine_customers_deduplication(spark, tmp_path):
    """
    Validation: Check if duplicate customers are correctly removed.
    Criteria: 
    - Input: 2 records with same customer_id.
    - Output: 1 unique record.
    """
    # 1. Setup Data with Duplicates
    data = [
        Row(customer_id="123", name="John Doe", signup_date="2023-01-01"),
        Row(customer_id="123", name="John Doe", signup_date="2023-01-01"), # Duplicate
        Row(customer_id="456", name="Jane Doe", signup_date="2023-02-01")
    ]
    df_raw = spark.createDataFrame(data)
    
    # 2. Execute Logic (Simulated)
    # Testamos a l√≥gica isolada de dropDuplicates usada no job Silver
    df_dedup = df_raw.dropDuplicates(["customer_id"])
    
    # 3. Assertions
    assert df_dedup.count() == 2
    ids = [r.customer_id for r in df_dedup.collect()]
    assert "123" in ids
    assert "456" in ids
```

## 7.3 GitHub Actions Pipeline
Foi criado um workflow que executa automaticamente a cada `git push`.

**A√ß√£o:** Crie o arquivo `ci.yaml` dentro dele:
**Arquivo:** `.github/workflows/ci.yaml`

```yaml
name: CI Pipeline

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"

    - name: Install Java (for Spark)
      uses: actions/setup-java@v3
      with:
        distribution: 'temurin'
        java-version: '17'

    - name: Install Poetry
      run: |
        curl -sSL https://install.python-poetry.org | python3 -

    - name: Install Dependencies
      run: |
        poetry install

    - name: Run Smoke Tests
      run: |
        poetry run pytest tests/test_smoke.py
```

### üõë Checkpoint: Execu√ß√£o & Versionamento (v0.7.0)


1.  **Garanta a estrutura:**
    Verifique se as pastas `tests/` e `.github/workflows` foram criadas corretamente na etapa 7.0.

2.  **Validar Tests Localmente:**
    Antes de subir, teste na sua m√°quina.
    ```bash
    poetry run pytest tests/test_smoke.py
    ```
    *Resultado esperado: 1 passed in X.Xs*

3.  **Commitar Mudan√ßas:**
    ```bash
    git add .
    git commit -m "feat: implement ci/cd pipeline"
    ```

4.  **Lan√ßar Vers√£o v0.7.0:**
    ```bash
    make bump-minor
    ```

5.  **Sincronizar com Reposit√≥rio Remoto e Disparar CI:**
    Ao fazer o push, v√° at√© a aba "Actions" no seu GitHub para ver o pipeline rodando.
    ```bash
    git push origin main
    git push --tags
    ```

---

# Cap√≠tulo 8: Pr√≥ximos Passos (Roadmap)

Features planejadas para vers√µes futuras:
1.  **Containeriza√ß√£o (Docker):** Criar imagem otimizada para execu√ß√£o em Kubernetes.
2.  **Externaliza√ß√£o (Pydantic):** Migrar configura√ß√µes hardcoded para `config.yaml` validado.
